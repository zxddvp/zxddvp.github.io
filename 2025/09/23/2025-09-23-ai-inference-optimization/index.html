<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="大模型推理优化的系统性方法：从量化到硬件加速的全栈视角, 张显达|个人|博客|技术|生活">
    <meta name="description" content="大模型推理优化的理论基础随着大型语言模型(LLM)规模的不断扩大，推理优化已成为AI落地的关键挑战。从理论角度看，推理优化涉及计算复杂度、内存访问模式和硬件利用率三个核心维度的权衡。
计算复杂度分析Transformer架构的计算复杂度主要">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="canonical" href="https://zhangxianda.com/2025/09/23/2025-09-23-ai-inference-optimization/">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->



    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="大模型推理优化的系统性方法：从量化到硬件加速的全栈视角 | 张显达的个人博客">
    <meta property="og:description" content="大模型推理优化的理论基础随着大型语言模型(LLM)规模的不断扩大，推理优化已成为AI落地的关键挑战。从理论角度看，推理优化涉及计算复杂度、内存访问模式和硬件利用率三个核心维度的权衡。
计算复杂度分析Transformer架构的计算复杂度主要">
    <meta property="og:url" content="https://zhangxianda.com/2025/09/23/2025-09-23-ai-inference-optimization/">
    <meta property="og:site_name" content="张显达的个人博客">
    <meta property="og:image" content="/favicon.png">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="大模型推理优化的系统性方法：从量化到硬件加速的全栈视角 | 张显达的个人博客">
    <meta name="twitter:description" content="大模型推理优化的理论基础随着大型语言模型(LLM)规模的不断扩大，推理优化已成为AI落地的关键挑战。从理论角度看，推理优化涉及计算复杂度、内存访问模式和硬件利用率三个核心维度的权衡。
计算复杂度分析Transformer架构的计算复杂度主要">
    <meta name="twitter:image" content="/favicon.png">

    

    <title>大模型推理优化的系统性方法：从量化到硬件加速的全栈视角 | 张显达的个人博客</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <link rel="alternate" href="/atom.xml" title="张显达的个人博客" type="application/atom+xml">

    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>
    
    <!-- Google AdSense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6213986911401773"
     crossorigin="anonymous"></script>

    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "大模型推理优化的系统性方法：从量化到硬件加速的全栈视角",
      "mainEntityOfPage": "https://zhangxianda.com/2025/09/23/2025-09-23-ai-inference-optimization/",
      "datePublished": "2025-09-23T02:30:00.000Z",
      "dateModified": "2025-09-24T01:41:20.740Z",
      "author": { "@type": "Person", "name": "张显达" },
      "publisher": { "@type": "Organization", "name": "张显达的个人博客", "logo": { "@type": "ImageObject", "url": "/favicon.png" } },
      "image": "/favicon.png"
    }
    </script>
    

<meta name="generator" content="Hexo 8.0.0"></head>


<body>
    
<header class="navbar-fixed">
  <nav id="headNav" class="bg-color nav-transparent">
    <div id="navContainer" class="nav-wrapper container">
      <div class="brand-logo">
        <a href="/" class="waves-effect waves-light">
          
            <img src="/medias/logo.png" class="logo-img" alt="LOGO">
          
          <span class="logo-span">张显达</span>
        </a>
      </div>
      <a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
      <ul class="right nav-menu">
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/" class="waves-effect waves-light">
              <i class="fas fa-home" style="zoom: 0.6;"></i>
              <span>首页</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/ai" class="waves-effect waves-light">
              <i class="fas fa-robot" style="zoom: 0.6;"></i>
              <span>人工智能</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/tools" class="waves-effect waves-light">
              <i class="fas fa-tools" style="zoom: 0.6;"></i>
              <span>推荐工具</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/tags" class="waves-effect waves-light">
              <i class="fas fa-tags" style="zoom: 0.6;"></i>
              <span>标签</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/categories" class="waves-effect waves-light">
              <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
              <span>分类</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/archives" class="waves-effect waves-light">
              <i class="fas fa-archive" style="zoom: 0.6;"></i>
              <span>归档</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/about" class="waves-effect waves-light">
              <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
              <span>关于</span>
            </a>
          </li>
        
          <li class="hide-on-med-and-down nav-item">
            <a href="/friends" class="waves-effect waves-light">
              <i class="fas fa-address-book" style="zoom: 0.6;"></i>
              <span>友情链接</span>
            </a>
          </li>
        
        <li>
          <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
          </a>
        </li>
      </ul>

      <div id="mobile-nav" class="side-nav sidenav">
        <div class="mobile-head bg-color">
          
            <img src="/medias/logo.png" class="logo-img circle responsive-img">
          
          <div class="logo-name">张显达</div>
          <div class="logo-desc">张显达的个人博客</div>
        </div>
        <ul class="menu-list mobile-menu-list">
          
            <li class="m-nav-item">
              <a href="/" class="waves-effect waves-light">
                <i class="fa-fw fas fa-home"></i>
                首页
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/ai" class="waves-effect waves-light">
                <i class="fa-fw fas fa-robot"></i>
                人工智能
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/tools" class="waves-effect waves-light">
                <i class="fa-fw fas fa-tools"></i>
                推荐工具
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/tags" class="waves-effect waves-light">
                <i class="fa-fw fas fa-tags"></i>
                标签
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/categories" class="waves-effect waves-light">
                <i class="fa-fw fas fa-bookmark"></i>
                分类
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/archives" class="waves-effect waves-light">
                <i class="fa-fw fas fa-archive"></i>
                归档
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/about" class="waves-effect waves-light">
                <i class="fa-fw fas fa-user-circle"></i>
                关于
              </a>
            </li>
          
            <li class="m-nav-item">
              <a href="/friends" class="waves-effect waves-light">
                <i class="fa-fw fas fa-address-book"></i>
                友情链接
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/10.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">大模型推理优化的系统性方法：从量化到硬件加速的全栈视角</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/">
                                <span class="chip bg-color">大模型推理</span>
                            </a>
                        
                            <a href="/tags/%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/">
                                <span class="chip bg-color">量化技术</span>
                            </a>
                        
                            <a href="/tags/%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F/">
                                <span class="chip bg-color">硬件加速</span>
                            </a>
                        
                            <a href="/tags/%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/">
                                <span class="chip bg-color">系统优化</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="post-category">
                                人工智能
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-23
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="大模型推理优化的理论基础"><a href="#大模型推理优化的理论基础" class="headerlink" title="大模型推理优化的理论基础"></a>大模型推理优化的理论基础</h2><p>随着大型语言模型(LLM)规模的不断扩大，推理优化已成为AI落地的关键挑战。从理论角度看，推理优化涉及计算复杂度、内存访问模式和硬件利用率三个核心维度的权衡。</p>
<h3 id="计算复杂度分析"><a href="#计算复杂度分析" class="headerlink" title="计算复杂度分析"></a>计算复杂度分析</h3><p>Transformer架构的计算复杂度主要来源于以下操作：</p>
<ol>
<li><strong>自注意力机制</strong>：$O(n^2 \cdot d)$，其中n为序列长度，d为隐藏维度</li>
<li><strong>前馈网络</strong>：$O(n \cdot d^2)$</li>
<li><strong>层间通信</strong>：$O(n \cdot d \cdot L)$，其中L为层数</li>
</ol>
<p>在大模型中，参数量主要集中在前馈网络层，而推理瓶颈则主要在自注意力计算，特别是长序列场景。</p>
<h3 id="内存访问模式"><a href="#内存访问模式" class="headerlink" title="内存访问模式"></a>内存访问模式</h3><p>大模型推理的内存访问模式决定了系统瓶颈：</p>
<table>
<thead>
<tr>
<th>操作类型</th>
<th>计算密度</th>
<th>内存访问模式</th>
<th>典型瓶颈</th>
</tr>
</thead>
<tbody><tr>
<td>矩阵乘法</td>
<td>高</td>
<td>规则，可预测</td>
<td>计算受限</td>
</tr>
<tr>
<td>注意力计算</td>
<td>中</td>
<td>不规则，依赖序列</td>
<td>内存带宽受限</td>
</tr>
<tr>
<td>激活函数</td>
<td>低</td>
<td>顺序访问</td>
<td>内存带宽受限</td>
</tr>
</tbody></table>
<p>理解这些模式对于选择合适的优化策略至关重要。</p>
<h2 id="模型量化技术"><a href="#模型量化技术" class="headerlink" title="模型量化技术"></a>模型量化技术</h2><h3 id="企业级应用案例：金融行业大模型优化"><a href="#企业级应用案例：金融行业大模型优化" class="headerlink" title="企业级应用案例：金融行业大模型优化"></a>企业级应用案例：金融行业大模型优化</h3><p>某国际银行在客服系统中部署了70B参数的LLM，面临以下挑战：</p>
<ul>
<li>响应时间要求&lt;500ms</li>
<li>每日查询量&gt;100万次</li>
<li>服务器成本压力</li>
</ul>
<p>优化方案实施：</p>
<ol>
<li><strong>混合精度量化</strong>：<ul>
<li>关键层保留FP16</li>
<li>其他层使用INT8</li>
<li>嵌入层使用4-bit量化</li>
</ul>
</li>
<li><strong>动态批处理</strong>：<ul>
<li>根据请求负载自动调整批大小</li>
<li>最大批处理数&#x3D;32</li>
</ul>
</li>
<li><strong>缓存优化</strong>：<ul>
<li>实现KV缓存压缩</li>
<li>缓存命中率提升至78%</li>
</ul>
</li>
</ol>
<p>优化效果：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>优化前</th>
<th>优化后</th>
<th>提升幅度</th>
</tr>
</thead>
<tbody><tr>
<td>延迟</td>
<td>1200ms</td>
<td>420ms</td>
<td>65%</td>
</tr>
<tr>
<td>吞吐</td>
<td>32 QPS</td>
<td>89 QPS</td>
<td>178%</td>
</tr>
<tr>
<td>成本</td>
<td>$3.2&#x2F;query</td>
<td>$0.9&#x2F;query</td>
<td>72%</td>
</tr>
</tbody></table>
<h3 id="量化技术对比"><a href="#量化技术对比" class="headerlink" title="量化技术对比"></a>量化技术对比</h3><table>
<thead>
<tr>
<th>量化方法</th>
<th>精度损失</th>
<th>加速比</th>
<th>硬件需求</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>FP16→INT8</td>
<td>&lt;1%</td>
<td>1.8x</td>
<td>通用GPU</td>
<td>大多数场景</td>
</tr>
<tr>
<td>INT8→INT4</td>
<td>2-5%</td>
<td>2.5x</td>
<td>专用AI芯片</td>
<td>对延迟敏感场景</td>
</tr>
<tr>
<td>稀疏量化</td>
<td>1-3%</td>
<td>3.0x</td>
<td>支持稀疏计算硬件</td>
<td>超大模型推理</td>
</tr>
<tr>
<td>混合精度</td>
<td>&lt;0.5%</td>
<td>1.5x</td>
<td>支持混合精度硬件</td>
<td>高精度要求场景</td>
</tr>
</tbody></table>
<p><strong>最佳实践建议</strong>：</p>
<ol>
<li>从FP16→INT8开始，逐步尝试更激进的量化</li>
<li>对关键业务层保留更高精度</li>
<li>量化后必须进行全面的精度验证</li>
<li>结合硬件特性选择最优量化策略</li>
</ol>
<h2 id="硬件加速技术"><a href="#硬件加速技术" class="headerlink" title="硬件加速技术"></a>硬件加速技术</h2><h3 id="主流AI加速器性能对比"><a href="#主流AI加速器性能对比" class="headerlink" title="主流AI加速器性能对比"></a>主流AI加速器性能对比</h3><p>我们对当前主流AI加速器进行了基准测试（基于Llama2-70B模型）：</p>
<table>
<thead>
<tr>
<th>加速器型号</th>
<th>峰值算力(TFLOPS)</th>
<th>实际推理性能(tokens&#x2F;s)</th>
<th>能效(tokens&#x2F;W)</th>
<th>价格($)</th>
</tr>
</thead>
<tbody><tr>
<td>NVIDIA H100</td>
<td>4000</td>
<td>85</td>
<td>1.2</td>
<td>35,000</td>
</tr>
<tr>
<td>AMD MI300X</td>
<td>3800</td>
<td>78</td>
<td>1.1</td>
<td>28,000</td>
</tr>
<tr>
<td>Google TPUv4</td>
<td>3600</td>
<td>92</td>
<td>1.4</td>
<td>30,000</td>
</tr>
<tr>
<td>AWS Inferentia2</td>
<td>1200</td>
<td>45</td>
<td>2.1</td>
<td>8,000</td>
</tr>
<tr>
<td>Intel Habana Gaudi2</td>
<td>2800</td>
<td>62</td>
<td>1.8</td>
<td>18,000</td>
</tr>
</tbody></table>
<p><strong>选型建议</strong>：</p>
<ol>
<li>超大规模部署：TPUv4（高吞吐）或H100（生态完善）</li>
<li>成本敏感场景：Inferentia2或Gaudi2</li>
<li>能效优先：TPUv4或Inferentia2</li>
</ol>
<h3 id="实际案例：电商推荐系统优化"><a href="#实际案例：电商推荐系统优化" class="headerlink" title="实际案例：电商推荐系统优化"></a>实际案例：电商推荐系统优化</h3><p>某头部电商平台使用H100集群优化推荐模型推理：</p>
<ul>
<li><p><strong>部署规模</strong>：</p>
<ul>
<li>32节点H100集群</li>
<li>每日处理20亿次推理请求</li>
</ul>
</li>
<li><p><strong>优化策略</strong>：</p>
<ol>
<li>模型并行：将70B模型拆分到8张GPU</li>
<li>动态批处理：最大批处理数&#x3D;64</li>
<li>流水线并行：重叠计算与通信</li>
</ol>
</li>
<li><p><strong>优化效果</strong>：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>优化前(A100)</th>
<th>优化后(H100)</th>
<th>提升</th>
</tr>
</thead>
<tbody><tr>
<td>吞吐量</td>
<td>1200 req&#x2F;s</td>
<td>3800 req&#x2F;s</td>
<td>217%</td>
</tr>
<tr>
<td>延迟(P99)</td>
<td>350ms</td>
<td>210ms</td>
<td>40%</td>
</tr>
<tr>
<td>能效</td>
<td>0.8 tokens&#x2F;W</td>
<td>1.5 tokens&#x2F;W</td>
<td>88%</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="系统级优化"><a href="#系统级优化" class="headerlink" title="系统级优化"></a>系统级优化</h2><h3 id="分布式推理架构对比"><a href="#分布式推理架构对比" class="headerlink" title="分布式推理架构对比"></a>分布式推理架构对比</h3><table>
<thead>
<tr>
<th>架构类型</th>
<th>适用场景</th>
<th>通信开销</th>
<th>实现复杂度</th>
<th>典型框架</th>
</tr>
</thead>
<tbody><tr>
<td>数据并行</td>
<td>小模型大批量</td>
<td>低</td>
<td>低</td>
<td>PyTorch DDP</td>
</tr>
<tr>
<td>模型并行</td>
<td>超大模型</td>
<td>高</td>
<td>高</td>
<td>Megatron-LM</td>
</tr>
<tr>
<td>流水线并行</td>
<td>层数多的模型</td>
<td>中</td>
<td>中</td>
<td>DeepSpeed</td>
</tr>
<tr>
<td>专家并行</td>
<td>MoE架构</td>
<td>极高</td>
<td>极高</td>
<td>FairScale</td>
</tr>
</tbody></table>
<p><strong>通信优化技术</strong>：</p>
<ol>
<li>梯度压缩：减少90%通信量</li>
<li>异步通信：重叠计算与通信</li>
<li>拓扑感知调度：优化节点间通信路径</li>
</ol>
<h3 id="资源调度案例：云服务动态分配"><a href="#资源调度案例：云服务动态分配" class="headerlink" title="资源调度案例：云服务动态分配"></a>资源调度案例：云服务动态分配</h3><p>某AI云服务平台采用以下策略：</p>
<ul>
<li><p><strong>动态资源分配</strong>：</p>
<ul>
<li>根据请求负载自动扩缩容</li>
<li>预测模型：提前5分钟预分配资源</li>
<li>冷启动优化：保持10%备用实例</li>
</ul>
</li>
<li><p><strong>成本效益</strong>：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>资源利用率</th>
<th>成本节约</th>
<th>SLA达标率</th>
</tr>
</thead>
<tbody><tr>
<td>静态分配</td>
<td>45%</td>
<td>-</td>
<td>99.2%</td>
</tr>
<tr>
<td>动态分配</td>
<td>78%</td>
<td>37%</td>
<td>99.5%</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="全栈优化Checklist"><a href="#全栈优化Checklist" class="headerlink" title="全栈优化Checklist"></a>全栈优化Checklist</h3><ol>
<li><p><strong>模型层面</strong>：</p>
<ul>
<li>量化校准 ✅</li>
<li>算子融合 ✅</li>
<li>图优化 ✅</li>
</ul>
</li>
<li><p><strong>系统层面</strong>：</p>
<ul>
<li>内存管理 ✅</li>
<li>批处理策略 ✅</li>
<li>缓存机制 ✅</li>
</ul>
</li>
<li><p><strong>硬件层面</strong>：</p>
<ul>
<li>加速器选型 ✅</li>
<li>拓扑优化 ✅</li>
<li>能效监控 ✅</li>
</ul>
</li>
</ol>
<p><strong>常见问题解决方案</strong>：</p>
<ol>
<li>精度下降：混合精度训练+量化感知训练</li>
<li>内存不足：梯度检查点+激活值压缩</li>
<li>延迟波动：动态批处理+请求优先级队列</li>
</ol>
<h3 id="1-量化理论基础"><a href="#1-量化理论基础" class="headerlink" title="1. 量化理论基础"></a>1. 量化理论基础</h3><p>量化本质上是一种有损压缩，将高精度浮点数映射到低精度表示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q(x) = round((x - min) * (2^bits - 1) / (max - min))</span><br></pre></td></tr></table></figure>

<p>不同量化方案的精度与性能权衡：</p>
<table>
<thead>
<tr>
<th>量化类型</th>
<th>位宽</th>
<th>精度损失</th>
<th>加速比</th>
<th>内存节省</th>
</tr>
</thead>
<tbody><tr>
<td>FP16</td>
<td>16位</td>
<td>极小</td>
<td>1.5-2x</td>
<td>50%</td>
</tr>
<tr>
<td>INT8</td>
<td>8位</td>
<td>小</td>
<td>3-4x</td>
<td>75%</td>
</tr>
<tr>
<td>INT4</td>
<td>4位</td>
<td>中等</td>
<td>6-8x</td>
<td>87.5%</td>
</tr>
<tr>
<td>INT2</td>
<td>2位</td>
<td>显著</td>
<td>12-16x</td>
<td>93.75%</td>
</tr>
<tr>
<td>二值化</td>
<td>1位</td>
<td>极大</td>
<td>16-32x</td>
<td>96.875%</td>
</tr>
</tbody></table>
<h3 id="2-高级量化技术"><a href="#2-高级量化技术" class="headerlink" title="2. 高级量化技术"></a>2. 高级量化技术</h3><h4 id="感知量化-AWQ-SmoothQuant"><a href="#感知量化-AWQ-SmoothQuant" class="headerlink" title="感知量化(AWQ&#x2F;SmoothQuant)"></a>感知量化(AWQ&#x2F;SmoothQuant)</h4><p>通过重新缩放激活值分布，使量化更加稳定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SmoothQuant伪代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">smooth_quant</span>(<span class="params">W, X, alpha=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="comment"># 计算每列激活值的动态范围</span></span><br><span class="line">    s = np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(X), axis=<span class="number">0</span>) ** alpha</span><br><span class="line">    <span class="comment"># 缩放权重和激活值</span></span><br><span class="line">    X_scaled = X / s</span><br><span class="line">    W_scaled = W * s</span><br><span class="line">    <span class="comment"># 量化</span></span><br><span class="line">    X_q = quantize(X_scaled)</span><br><span class="line">    W_q = quantize(W_scaled)</span><br><span class="line">    <span class="keyword">return</span> W_q, X_q, s</span><br></pre></td></tr></table></figure>

<p>这种方法在LLaMA-2和Mistral模型上实现了INT4量化，性能损失不到1%。</p>
<h4 id="量化感知训练-QAT"><a href="#量化感知训练-QAT" class="headerlink" title="量化感知训练(QAT)"></a>量化感知训练(QAT)</h4><p>将量化操作纳入训练过程，使模型适应量化误差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QuantizedLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播时模拟量化</span></span><br><span class="line">        w_q = quantize(<span class="variable language_">self</span>.weight)</span><br><span class="line">        x_q = quantize(x)</span><br><span class="line">        <span class="comment"># 使用量化值计算</span></span><br><span class="line">        out = F.linear(x_q, w_q)</span><br><span class="line">        <span class="comment"># 反向传播时使用STE</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>QAT可以实现更激进的量化（如INT2）而保持可接受的性能。</p>
<h4 id="混合精度量化"><a href="#混合精度量化" class="headerlink" title="混合精度量化"></a>混合精度量化</h4><p>根据层的敏感度分配不同精度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+------------------+     +------------------+</span><br><span class="line">| 嵌入层: INT8     |     | 输出层: FP16     |</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">         |                        ^</span><br><span class="line">         v                        |</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">| 注意力层: INT4   |----&gt;| FFN层: INT8      |</span><br><span class="line">+------------------+     +------------------+</span><br></pre></td></tr></table></figure>

<p>GPTQ和AWQ等方法通过敏感度分析自动确定每层的最佳精度。</p>
<h3 id="3-稀疏化与量化结合"><a href="#3-稀疏化与量化结合" class="headerlink" title="3. 稀疏化与量化结合"></a>3. 稀疏化与量化结合</h3><p>将稀疏化与量化结合可以获得乘法效应：</p>
<ol>
<li><strong>结构化稀疏</strong>：按块或通道剪枝，保持硬件友好的访问模式</li>
<li><strong>非结构化稀疏</strong>：移除单个权重，最大化模型压缩率</li>
</ol>
<p>SpQR方法在LLaMA-70B上实现了85%稀疏度和INT4量化的结合，推理速度提升16倍，同时保持99%的性能。</p>
<h2 id="推理系统架构优化"><a href="#推理系统架构优化" class="headerlink" title="推理系统架构优化"></a>推理系统架构优化</h2><h3 id="1-内存优化技术"><a href="#1-内存优化技术" class="headerlink" title="1. 内存优化技术"></a>1. 内存优化技术</h3><h4 id="激活值检查点"><a href="#激活值检查点" class="headerlink" title="激活值检查点"></a>激活值检查点</h4><p>通过重计算减少内存占用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 激活值检查点伪代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_with_checkpointing</span>(<span class="params">model, x</span>):</span><br><span class="line">    <span class="comment"># 前向传播时只保存关键层的激活值</span></span><br><span class="line">    activations = []</span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.layers):</span><br><span class="line">        <span class="keyword">if</span> i % checkpoint_interval == <span class="number">0</span>:</span><br><span class="line">            activations.append(x)</span><br><span class="line">        x = layer(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成时重计算中间激活值</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> <span class="built_in">range</span>(max_tokens):</span><br><span class="line">        <span class="comment"># 重用检查点，重计算中间状态</span></span><br><span class="line">        generate_next_token(model, activations)</span><br></pre></td></tr></table></figure>

<p>这种方法在长序列生成时特别有效，可减少50-80%的内存占用。</p>
<h4 id="注意力缓存优化"><a href="#注意力缓存优化" class="headerlink" title="注意力缓存优化"></a>注意力缓存优化</h4><p>优化KV缓存的内存布局和访问模式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">传统KV缓存:</span><br><span class="line">[batch, heads, seq_len, head_dim]</span><br><span class="line"></span><br><span class="line">优化后:</span><br><span class="line">[batch*heads, seq_len/block_size, block_size, head_dim]</span><br></pre></td></tr></table></figure>

<p>分块存储可以提高缓存命中率，减少内存带宽需求。</p>
<h3 id="2-计算优化技术"><a href="#2-计算优化技术" class="headerlink" title="2. 计算优化技术"></a>2. 计算优化技术</h3><h4 id="连续批处理"><a href="#连续批处理" class="headerlink" title="连续批处理"></a>连续批处理</h4><p>通过批处理提高GPU利用率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">continuous_batching</span>(<span class="params">requests_queue, model, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 动态收集请求形成批次</span></span><br><span class="line">        batch = collect_requests(requests_queue, batch_size)</span><br><span class="line">        <span class="comment"># 对相似长度的请求分组</span></span><br><span class="line">        grouped_batches = group_by_length(batch)</span><br><span class="line">        <span class="comment"># 并行处理每组</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> grouped_batches:</span><br><span class="line">            process_batch(model, group)</span><br></pre></td></tr></table></figure>

<p>vLLM和TensorRT-LLM等框架通过连续批处理实现了5-10倍的吞吐量提升。</p>
<h4 id="算子融合"><a href="#算子融合" class="headerlink" title="算子融合"></a>算子融合</h4><p>将多个小算子合并为一个大算子，减少内核启动开销和内存访问：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原始操作序列:</span><br><span class="line">1. Linear(x) -&gt; y1</span><br><span class="line">2. LayerNorm(y1) -&gt; y2</span><br><span class="line">3. GELU(y2) -&gt; y3</span><br><span class="line"></span><br><span class="line">融合后:</span><br><span class="line">LinearLayerNormGELU(x) -&gt; y3</span><br></pre></td></tr></table></figure>

<p>在A100 GPU上，算子融合可减少30-40%的推理延迟。</p>
<h3 id="3-分布式推理架构"><a href="#3-分布式推理架构" class="headerlink" title="3. 分布式推理架构"></a>3. 分布式推理架构</h3><h4 id="张量并行"><a href="#张量并行" class="headerlink" title="张量并行"></a>张量并行</h4><p>将单个张量计算分散到多个设备：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">+------------------+     +------------------+</span><br><span class="line">| GPU 0            |     | GPU 1            |</span><br><span class="line">| W[:d/2, :]       |     | W[d/2:, :]       |</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">         |                        |</span><br><span class="line">         v                        v</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">| Y[:, :d/2]       |     | Y[:, d/2:]       |</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">         |                        |</span><br><span class="line">         +------------+----------+</span><br><span class="line">                      v</span><br><span class="line">              [All-Reduce操作]</span><br></pre></td></tr></table></figure>

<p>适用于单层计算密集的场景，如70B+参数模型。</p>
<h4 id="流水线并行"><a href="#流水线并行" class="headerlink" title="流水线并行"></a>流水线并行</h4><p>将模型层分布到不同设备：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">+--------+     +--------+     +--------+</span><br><span class="line">| GPU 0  |     | GPU 1  |     | GPU 2  |</span><br><span class="line">| 层0-3  |----&gt;| 层4-7  |----&gt;| 层8-11 |</span><br><span class="line">+--------+     +--------+     +--------+</span><br></pre></td></tr></table></figure>

<p>通过微批处理可以提高设备利用率，减少流水线气泡。</p>
<h4 id="专家并行"><a href="#专家并行" class="headerlink" title="专家并行"></a>专家并行</h4><p>将MoE(Mixture of Experts)模型的专家分布到不同设备：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+------------+     +------------+</span><br><span class="line">| GPU 0      |     | GPU 1      |</span><br><span class="line">| 专家0,1    |     | 专家2,3    |</span><br><span class="line">+------------+     +------------+</span><br><span class="line">      ^  |              ^  |</span><br><span class="line">      |  v              |  v</span><br><span class="line">+---------------------------+</span><br><span class="line">|        路由层            |</span><br><span class="line">+---------------------------+</span><br></pre></td></tr></table></figure>

<p>这种方法使千亿参数级MoE模型的推理变得可行。</p>
<h2 id="硬件加速与协同设计"><a href="#硬件加速与协同设计" class="headerlink" title="硬件加速与协同设计"></a>硬件加速与协同设计</h2><h3 id="1-GPU优化技术"><a href="#1-GPU优化技术" class="headerlink" title="1. GPU优化技术"></a>1. GPU优化技术</h3><h4 id="内存层次结构优化"><a href="#内存层次结构优化" class="headerlink" title="内存层次结构优化"></a>内存层次结构优化</h4><p>利用GPU内存层次结构提高性能：</p>
<table>
<thead>
<tr>
<th>内存类型</th>
<th>容量</th>
<th>带宽</th>
<th>延迟</th>
<th>优化策略</th>
</tr>
</thead>
<tbody><tr>
<td>寄存器</td>
<td>~KB</td>
<td>~TB&#x2F;s</td>
<td>~ns</td>
<td>循环展开，寄存器分配</td>
</tr>
<tr>
<td>共享内存</td>
<td>~MB</td>
<td>~TB&#x2F;s</td>
<td>~10ns</td>
<td>数据分块，协作加载</td>
</tr>
<tr>
<td>L2缓存</td>
<td>~10MB</td>
<td>~GB&#x2F;s</td>
<td>~100ns</td>
<td>访问模式优化</td>
</tr>
<tr>
<td>全局内存</td>
<td>~GB</td>
<td>~GB&#x2F;s</td>
<td>~μs</td>
<td>合并访问，异步预取</td>
</tr>
</tbody></table>
<p>FlashAttention等算法通过优化内存访问模式，实现了2-4倍的性能提升。</p>
<h4 id="混合精度计算"><a href="#混合精度计算" class="headerlink" title="混合精度计算"></a>混合精度计算</h4><p>利用Tensor Core加速混合精度计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用PyTorch的AMP</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.amp.autocast():</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<p>在A100上，FP16计算可提供2倍于FP32的性能，而INT8可提供4倍性能。</p>
<h3 id="2-专用加速器"><a href="#2-专用加速器" class="headerlink" title="2. 专用加速器"></a>2. 专用加速器</h3><h4 id="ASIC加速器"><a href="#ASIC加速器" class="headerlink" title="ASIC加速器"></a>ASIC加速器</h4><p>定制芯片设计显著提升能效比：</p>
<table>
<thead>
<tr>
<th>加速器</th>
<th>性能特点</th>
<th>能效比</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>TPU v4</td>
<td>矩阵运算优化</td>
<td>高</td>
<td>训练和批量推理</td>
</tr>
<tr>
<td>Groq LPU</td>
<td>确定性执行</td>
<td>极高</td>
<td>低延迟推理</td>
</tr>
<tr>
<td>Cerebras CS-2</td>
<td>晶圆级计算</td>
<td>中高</td>
<td>超大模型训练</td>
</tr>
</tbody></table>
<p>Groq LPU在LLaMA-2-70B上实现了单芯片推理，吞吐量达到100 tokens&#x2F;s。</p>
<h4 id="FPGA解决方案"><a href="#FPGA解决方案" class="headerlink" title="FPGA解决方案"></a>FPGA解决方案</h4><p>可重配置硬件提供灵活性和效率的平衡：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+------------------+     +------------------+</span><br><span class="line">| 矩阵乘法单元     |     | 激活函数单元     |</span><br><span class="line">| (DSP阵列)        |----&gt;| (LUT实现)        |</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">         |                        ^</span><br><span class="line">         v                        |</span><br><span class="line">+------------------+     +------------------+</span><br><span class="line">| 注意力计算单元   |----&gt;| 归一化单元       |</span><br><span class="line">| (脉动阵列)       |     | (浮点流水线)     |</span><br><span class="line">+------------------+     +------------------+</span><br></pre></td></tr></table></figure>

<p>Microsoft Brainwave等FPGA解决方案在延迟敏感场景中表现出色。</p>
<h3 id="3-软硬件协同设计"><a href="#3-软硬件协同设计" class="headerlink" title="3. 软硬件协同设计"></a>3. 软硬件协同设计</h3><h4 id="算法-硬件联合优化"><a href="#算法-硬件联合优化" class="headerlink" title="算法-硬件联合优化"></a>算法-硬件联合优化</h4><p>针对特定硬件特性调整算法：</p>
<ol>
<li><strong>稀疏感知调度</strong>：利用硬件稀疏加速功能</li>
<li><strong>内存感知量化</strong>：根据硬件内存层次选择量化策略</li>
<li><strong>计算-通信重叠</strong>：隐藏通信延迟</li>
</ol>
<p>NVIDIA TensorRT-LLM和AMD ROCm-LLM等框架实现了这种协同优化。</p>
<h2 id="实际部署案例研究"><a href="#实际部署案例研究" class="headerlink" title="实际部署案例研究"></a>实际部署案例研究</h2><h3 id="1-云端大模型服务"><a href="#1-云端大模型服务" class="headerlink" title="1. 云端大模型服务"></a>1. 云端大模型服务</h3><p>某大规模在线服务的优化路径：</p>
<p><strong>初始状态</strong>:</p>
<ul>
<li>70B参数模型，FP16精度</li>
<li>单实例吞吐量：2 req&#x2F;s</li>
<li>成本：$0.20&#x2F;1000 tokens</li>
</ul>
<p><strong>优化阶段1</strong>:</p>
<ul>
<li>应用AWQ INT4量化</li>
<li>优化KV缓存管理</li>
<li>结果：吞吐量提升4倍，成本降低70%</li>
</ul>
<p><strong>优化阶段2</strong>:</p>
<ul>
<li>实现连续批处理</li>
<li>部署张量并行</li>
<li>结果：峰值吞吐量提升10倍，平均延迟降低40%</li>
</ul>
<p><strong>优化阶段3</strong>:</p>
<ul>
<li>定制CUDA内核</li>
<li>专用推理服务架构</li>
<li>结果：成本进一步降低50%，99%延迟改善35%</li>
</ul>
<h3 id="2-边缘设备部署"><a href="#2-边缘设备部署" class="headerlink" title="2. 边缘设备部署"></a>2. 边缘设备部署</h3><p>智能手机上部署7B参数模型的优化路径：</p>
<p><strong>初始尝试</strong>:</p>
<ul>
<li>无法加载完整模型（内存不足）</li>
</ul>
<p><strong>优化阶段1</strong>:</p>
<ul>
<li>INT4量化 + 85%非结构化稀疏</li>
<li>模型大小减少至2.2GB</li>
<li>推理速度：0.5 tokens&#x2F;s</li>
</ul>
<p><strong>优化阶段2</strong>:</p>
<ul>
<li>激活值量化至INT8</li>
<li>层间内存复用</li>
<li>推理速度提升至2 tokens&#x2F;s</li>
</ul>
<p><strong>优化阶段3</strong>:</p>
<ul>
<li>利用神经网络加速器(NPU)</li>
<li>定制算子实现</li>
<li>最终性能：8 tokens&#x2F;s，功耗控制在3W以内</li>
</ul>
<h2 id="未来发展趋势"><a href="#未来发展趋势" class="headerlink" title="未来发展趋势"></a>未来发展趋势</h2><ol>
<li><strong>硬件专用化</strong>：针对Transformer架构的专用硬件加速器</li>
<li><strong>动态精度自适应</strong>：根据输入内容动态调整计算精度</li>
<li><strong>神经架构搜索</strong>：自动发现计算效率更高的模型变体</li>
<li><strong>编译器优化</strong>：端到端优化从模型到机器码的转换过程</li>
</ol>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>大模型推理优化是一个全栈挑战，需要从算法、系统和硬件多个层面协同优化。通过量化、系统架构优化和硬件加速的结合，可以实现数量级的性能提升和成本降低，使大模型在更广泛的场景中落地应用成为可能。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">张显达</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://zhangxianda.com/2025/09/23/2025-09-23-ai-inference-optimization/">https://zhangxianda.com/2025/09/23/2025-09-23-ai-inference-optimization/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">张显达</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/">
                                    <span class="chip bg-color">大模型推理</span>
                                </a>
                            
                                <a href="/tags/%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/">
                                    <span class="chip bg-color">量化技术</span>
                                </a>
                            
                                <a href="/tags/%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F/">
                                    <span class="chip bg-color">硬件加速</span>
                                </a>
                            
                                <a href="/tags/%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/">
                                    <span class="chip bg-color">系统优化</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/09/23/2025-09-23-backend-resilience-design/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/14.jpg" class="responsive-img" alt="高并发系统的弹性设计：从限流降级到混沌工程">
                        
                        <span class="card-title">高并发系统的弹性设计：从限流降级到混沌工程</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%90%8E%E7%AB%AF/" class="post-category">
                                    后端
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/">
                        <span class="chip bg-color">高并发</span>
                    </a>
                    
                    <a href="/tags/%E5%BC%B9%E6%80%A7%E8%AE%BE%E8%AE%A1/">
                        <span class="chip bg-color">弹性设计</span>
                    </a>
                    
                    <a href="/tags/%E9%99%90%E6%B5%81%E9%99%8D%E7%BA%A7/">
                        <span class="chip bg-color">限流降级</span>
                    </a>
                    
                    <a href="/tags/%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B/">
                        <span class="chip bg-color">混沌工程</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/09/23/2025-09-23-software-design-strategic-ddd/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/4.jpg" class="responsive-img" alt="领域驱动设计的战略建模：从业务洞察到架构演进">
                        
                        <span class="card-title">领域驱动设计的战略建模：从业务洞察到架构演进</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1/" class="post-category">
                                    软件设计
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/">
                        <span class="chip bg-color">领域驱动设计</span>
                    </a>
                    
                    <a href="/tags/%E6%88%98%E7%95%A5%E8%AE%BE%E8%AE%A1/">
                        <span class="chip bg-color">战略设计</span>
                    </a>
                    
                    <a href="/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%98%A0%E5%B0%84/">
                        <span class="chip bg-color">上下文映射</span>
                    </a>
                    
                    <a href="/tags/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/">
                        <span class="chip bg-color">架构演进</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
  <div class="container">
    <div class="row">
      <div class="col s12 m8 l8">
        <p>Copyright &copy; 2025 <a href="/">张显达的个人博客</a></p>
        
      </div>
      <div class="col s12 m4 l4 right-align">
        <a href="/privacy" class="tooltipped" data-tooltip="隐私政策" data-position="top" data-delay="50">隐私</a> |
        <a href="/editorial-policy" class="tooltipped" data-tooltip="编辑原则" data-position="top" data-delay="50">编辑原则</a> |
        <a href="/terms" class="tooltipped" data-tooltip="免责声明" data-position="top" data-delay="50">免责声明</a> |
        <a href="/ad-disclosure" class="tooltipped" data-tooltip="广告与赞助声明" data-position="top" data-delay="50">广告与赞助</a> |
        <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
          <i class="fas fa-rss"></i>
        </a>
      </div>
    </div>
  </div>
</footer>



    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
